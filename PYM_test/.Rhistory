}
}
cdpost_j <- sum(cd) + mixprior_j
if (burnin == 0) {
result$mixlik[m] <- mixlik_j
result$mixprior[m] <- mixprior_j
result$nonnormpost[m] <- result$mixlik[m] + result$mixprior[m]
result$cdpost[m] <- cdpost_j
}
###################### fourth step: permutation of the labeling and storing the results
perm <- sample(K)
if (burnin == 0) {
### storing the new values:
result$Mu[m, , perm] <- mu_j
result$Eta[m, perm] <- eta_j
result$S_alt_matrix[m, ] <- perm[S_alt_j]
result$Nk_matrix_alt[m, perm] <- Nk_alt_j
result$B[m, ] <- B_j
result$Sigma[m, , , perm] <- sigma_j
result$acc_rate <- sum(acc)/M
}
if ((burnin == 0) & (result$nonnormpost[m] > result$nonnormpost_mode_list[[K0_j]]$nonnormpost)) {
result$nonnormpost_mode_list[[K0_j]] <- list(nonnormpost = result$nonnormpost[m], mu = mu_j[,
Nk_alt_j != 0], bk = bk[, Nk_alt_j != 0], Bk = Bk[, , Nk_alt_j != 0], eta = eta_j[Nk_alt_j !=
0])
}
if ((burnin == 0) & (result$mixlik[m] > result$mixlik_mode_list[[K0_j]]$mixlik)) {
result$mixlik_mode_list[[K0_j]] <- list(mixlik = result$mixlik[m], mu = mu_j[, Nk_alt_j !=
0], Sigma = sigma_j[, , Nk_alt_j != 0], eta = eta_j[Nk_alt_j != 0])  ##Bettina fragen:mixture likelihood oder complete data likelihood nehmen?
}
m <- m + 1
}
lambda = FALSE
priorOnE0= FALSE
set.seed(seed)
print(seed)
K <- ncol(mu_0)  #number of components
N <- nrow(y)  #number of observations
r <- ncol(y)  #number of dimensions
##change!!!!
R <- apply(y, 2, function(x) diff(range(x)))
## initializing current values:
eta_0 <- rep(1/K, K)
sigma_0 <- array(0, dim = c(r, r, K))
mu_j <- matrix(0, r, K)
S_j <- rep(0, N)
C0_j <- matrix(0, r, r)
eta_j <- eta_0
sigma_j <- sigma_0
invsigma_j <- sigma_j
det_invsigma_j <- rep(0, K)
mu_j <- mu_0
S_j <- S_0
#B_j <- B_0
B_j <- lam_0
C0_j <- C0_0
#change
#invB0_j <- solve(B0)
invB0_j <- solve(B0k)
b0_j <- b0
Nk_j <- tabulate(S_j, K)
print(Nk_j)
e0_p <- 0
julia <- julia_setup()
julia_library("GibbsTypePriors")
julia_library("DataFrames")
julia_library("DataFramesMeta")
mat = compute_matrix(N, sigma, K )
## generating matrices for storing the draws:
result <- list(Eta = matrix(0, M, K), Mu = array(0, dim = c(M, r, K)), Sigma = array(0, dim = c(M,
r, r, K)), S_alt_matrix = matrix(0L, M, N), Nk_matrix_alt = matrix(0L, M, K), Nk_view = matrix(0L,
(M + burnin)/100 + 1, K), B = matrix(0, M, r), e0_vector = rep(0, M), mixlik = rep(0, M),
mixprior = rep(0, M), nonnormpost = rep(0, M), cdpost = rep(0, M), nonnormpost_mode_list = vector("list",
K), mixlik_mode_list = vector("list", K))
acc <- rep(FALSE, M)  #for storing the acceptance results in the MH step
## Initialising the storing matrices:
result$Mu[1, , ] <- mu_0
result$Eta[1, ] <- eta_0
result$S_alt_matrix[1, ] <- S_0
#result$B[1, ] <- B_0
result$B[1, ] <- lam_0
result$Nk_matrix_alt[1, ] <- Nk_j
for (k in 1:K) {
result$nonnormpost_mode_list[[k]] <- list(nonnormpost = -(10)^18)
result$mixlik_mode_list[[k]] <- list(mixlik = -(10)^18)
}
### constant parameters for every iteration:
p_gig <- nu - K/2
a_gig <- 2 * nu
gn <- g0 + K * c0
####################################################################################### simulation starts:
s <- 1
result$Nk_view[1, ] <- Nk_j
m <- 2
while (m <= M | m <= burnin) {
if (m == burnin) {
m <- 1
burnin <- 0
}
### relevant component specific quantities:
mean_yk <- matrix(0, r, K)
mean_yk <- sapply(1:K, function(k) colMeans(y[S_j == k, , drop = FALSE]))
if (sum(is.na(mean_yk)) > 0) {
## to catch the case if a group is empty: NA values are substituted by zeros
mean_yk[is.na(mean_yk)] <- 0
}
Nk_j <- tabulate(S_j, K)
if (!(m%%100)) {
cat("\n", m, " ", Nk_j)
s <- s + 1
result$Nk_view[s, ] <- Nk_j
}
Nk_alt_j <- Nk_j
S_alt_j <- S_j
K0_j <- sum(Nk_j != 0)  ##number of nonempty components
#################### first step: parameter simulation (conditional on classification S_j): (1a): Sample eta_j:
#ek <- e0 + Nk_j
#eta_j <- bayesm::rdirichlet(ek)
eta_j <- sample.pym_w(S_j, alpha = e0, sigma = sigma, H = K, mat)
#### (1b): sample Sigma^{-1} for each component k: calculate posterior moments ck and Ck and sample
#### from the inverted Wishart distribution:
Ck <- array(0, dim = c(r, r, K))
ck <- c0 + Nk_j/2
for (k in 1:K) {
if (Nk_j[k] != 0) {
# Bettina:
Ck[, , k] <- C0_j + 0.5 * crossprod(sweep(y[S_j == k, , drop = FALSE], 2, mu_j[,
k], FUN = "-"))
} else {
Ck[, , k] <- C0_j
}
sig <- rwishart(2 * ck[k], 0.5 * chol2inv(chol(Ck[, , k])))  #attention: rwishart(nu,v)(Rossi)=> nu=2*c0,v=0.5*C0, wishart(c0,C0) (FS)
sigma_j[, , k] <- sig$IW
invsigma_j[, , k] <- sig$W
det_invsigma_j[k] <- det(invsigma_j[, , k])
}
#### (1c): Sample mu_j for each component k:
Bk <- array(0, dim = c(r, r, K))
bk <- matrix(0, r, K)
invB0_j <- diag(1/((R^2) * B_j))
for (k in 1:K) {
Bk[, , k] <- chol2inv(chol(invB0_j + invsigma_j[, , k] * Nk_j[k]))
bk[, k] <- Bk[, , k] %*% (invB0_j %*% b0_j + invsigma_j[, , k] %*% mean_yk[, k] * Nk_j[k])
mu_j[, k] <- t(chol(Bk[, , k])) %*% rnorm(r) + bk[, k]
}
#################### second step: classification of observations (conditional on knowing the parameters) Bettina:
mat <- sapply(1:K, function(k) eta_j[k] * dmvnorm(y, mu_j[, k], sigma_j[, , k]))
S_j <- apply(mat, 1, function(x) sample(1:K, 1, prob = x))
Nk_j <- tabulate(S_j, K)
Nk_neu_j <- Nk_j
###################### third step: sample the hyperparameters (3a): sample the hyperparameter-vector B conditionally
###################### on mu_j[,k] and sigma_j[,,k]:
b_gig <- vector(length = r)
b_gig <- (rowSums((mu_j - b0_j)^2))/R^2
if (lambda == TRUE) {
for (l in 1:r) {
# accept/reject algorithm from Helga:
if (b_gig[l] < 1e-06) {
check <- 0
while (check == 0) {
ran <- 1/rgamma(1, shape = -p_gig, scale = 2/b_gig[l])
check <- (runif(1) < exp(-a_gig/2 * ran))
}
} else {
### random generator from package 'Runuran': Create distribution object for GIG distribution
distr <- udgig(theta = p_gig, psi = a_gig, chi = b_gig[l])  #b_gig=chi,a_gig=psi,p_gig=theta
## Generate generator object; use method PINV (inversion)
gen <- pinvd.new(distr)
## Draw a sample of size 1
ran <- ur(gen, 1)
}
B_j[l] <- ran
}
} else {
#B_j <- B_0
B_j <- lam_0
}
#### (3b): sample the hyperparameter C0 conditionally on sigma:
C0_j <- rwishart(2 * gn, 0.5 * chol2inv(chol(G0 + rowSums(invsigma_j, dims = 2))))$W  #from package 'bayesm'
#### (3c):assuming that the mean appearing in the normal prior on the group mean mu_k follows a
#### improper prior p(b0)=const, sample b0 from N(1/K*sum(mu_i);1/K*B0 )
B0 <- diag((R^2) * B_j)
b0_j <- mvrnorm(1, rowSums(mu_j)/K, 1/K * B0)
#### (3d): sample the hyperparameter e0 from p(e0|eta,a,b) via MH-step:
a_gam <- 10
b_gam <- a_gam  #e0~G(a_gam,b_gam*Kmax)
# b_gam=1/K
Kmax <- K
const <- Kmax * b_gam
## current value:
e0_j <- e0
if (priorOnE0 == T) {
## proposal value:
le0_p <- log(e0_j) + rnorm(1, 0, c_proposal)
e0_p <- exp(le0_p)
# likelihood ratio between the proposed value and the previous value:
eta_j[eta_j == 0] <- 10^(-50)
lalpha1 <- (e0_p - e0_j) * sum(log(eta_j)) + lgamma(K * e0_p) - lgamma(K * e0_j) - K *
(lgamma(e0_p) - lgamma(e0_j)) + (a_gam - 1) * (log(e0_p) - log(e0_j)) - (e0_p - e0_j) *
const + (log(e0_p) - log(e0_j))
alpha1 <- min(exp(lalpha1), 1)
##
alpha2 <- runif(1)
## the proposed value is accepted with probability alpha2
if (alpha2 <= alpha1) {
e0_j <- e0_p
if (burnin == 0) {
acc[m] <- TRUE
}
}
}
result$e0_vector[m] <- e0_j
e0 <- e0_j
#### additional step (3e): evaluating the mixture likelihood and the complete-data posterior
## evaluating the mixture likelihood:
mat_neu <- sapply(1:K, function(k) eta_j[k] * dmvnorm(y, mu_j[, k], sigma_j[, , k]))
mixlik_j <- sum(log(rowSums(mat_neu)))
## evaluating the mixture prior:
mixprior_j <- log(MCMCpack::ddirichlet(as.vector(eta_j), rep(e0, K))) + sum(dmvnorm(t(mu_j),
b0, diag((R^2) * B_j), log = TRUE)) + sum(sapply(1:K, function(k) lndIWishart(2 * c0,
0.5 * C0_j, sigma_j[, , k]))) + lndIWishart(2 * g0, 0.5 * G0, C0_j) + dgamma(e0, shape = a_gam,
scale = 1/b_gam, log = TRUE) + sum(dgamma(B_j, shape = nu, scale = 1/nu, log = TRUE))
## evaluating the nonnormalized complete-data posterior:
cd <- c()
for (k in 1:K) {
if (sum(S_j == k)) {
cd[S_j == k] <- dmvnorm(y[S_j == k, ], mu_j[, k], sigma_j[, , k], log = TRUE)
}
}
cdpost_j <- sum(cd) + mixprior_j
if (burnin == 0) {
result$mixlik[m] <- mixlik_j
result$mixprior[m] <- mixprior_j
result$nonnormpost[m] <- result$mixlik[m] + result$mixprior[m]
result$cdpost[m] <- cdpost_j
}
###################### fourth step: permutation of the labeling and storing the results
perm <- sample(K)
if (burnin == 0) {
### storing the new values:
result$Mu[m, , perm] <- mu_j
result$Eta[m, perm] <- eta_j
result$S_alt_matrix[m, ] <- perm[S_alt_j]
result$Nk_matrix_alt[m, perm] <- Nk_alt_j
result$B[m, ] <- B_j
result$Sigma[m, , , perm] <- sigma_j
result$acc_rate <- sum(acc)/M
}
if ((burnin == 0) & (result$nonnormpost[m] > result$nonnormpost_mode_list[[K0_j]]$nonnormpost)) {
result$nonnormpost_mode_list[[K0_j]] <- list(nonnormpost = result$nonnormpost[m], mu = mu_j[,
Nk_alt_j != 0], bk = bk[, Nk_alt_j != 0], Bk = Bk[, , Nk_alt_j != 0], eta = eta_j[Nk_alt_j !=
0])
}
if ((burnin == 0) & (result$mixlik[m] > result$mixlik_mode_list[[K0_j]]$mixlik)) {
result$mixlik_mode_list[[K0_j]] <- list(mixlik = result$mixlik[m], mu = mu_j[, Nk_alt_j !=
0], Sigma = sigma_j[, , Nk_alt_j != 0], eta = eta_j[Nk_alt_j != 0])  ##Bettina fragen:mixture likelihood oder complete data likelihood nehmen?
}
m <- m + 1
}
result$Eta
result$Mu
S_j
table()
table(S_j)
eta_j
N_kj
Nk_j
#rm(list=ls())
## read sources
source("~/Documents/GitHub/BNPconsistency/scripts_for_figures/Code_SP_Mix/Estimation_SpMix.R")
source("~/Documents/GitHub/BNPconsistency/scripts_for_figures/Gibbs_sampling_function.R")
#rm(list=ls())
## read sources
source("~/Documents/GitHub/BNPconsistency/scripts_for_figures/Code_SP_Mix/Estimation_SpMix.R")
source("~/Documents/GitHub/BNPconsistency/scripts_for_figures/Gibbs_sampling_function.R")
require(tidyr)
require(e1071)
require(mclust)
require(MASS)
require(bayesm)
require(MCMCpack)
require(mvtnorm)
require(Runuran)
require(flexclust)
library(gridExtra)
library(cowplot)
library(ggplot2)
loadRData <- function(fileName){
#loads an RData file, and returns it
load(fileName)
get(ls()[ls() != "fileName"])
}
MCMC_function <- function(data, e0=0.01, K, M, burnin,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0)
loadRData <- function(fileName){
#loads an RData file, and returns it
load(fileName)
get(ls()[ls() != "fileName"])
}
ds_list<- c("~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_200.RData",
"~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_2000.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20000.RData")
ds_list<- c("~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_200.RData",
"~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_2000.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20000.RData")
data =  loadRData(ds_list[1])
data =  loadRData(ds_list[1])
df  = MCMC_function(data, e0=1, K=10, M=1000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
df
df
data
MCMC_function(data, e0=1, K=10, M=1000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
M
df  = MCMC_function(data, e0=1, K=10, M=1000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
data =  loadRData(ds_list[1])
e0=1
K=10
M=1000
burnin=200
seed_ = 1000
priorOnE0 = FALSE
sigma_py = 0.25
print(seed_)
y <- as.matrix(data$y)
Mmax <- M + burnin
## read dimensions of data:
r <- length(y[1, ])
N <- length(y[, 1])
## Dirichlet parameter for the mixture weights
## variance of the normal proposal for the MH step for estimating e0
c_proposal <- 0.8
R <- apply(y, 2, function(x) diff(range(x)))
## prior on Sigma_k
c0 <- 2.5 + (r - 1)/2
C0 <- 0.75 * cov(y)
g0 <- 0.5 + (r - 1)/2
G0 <- 100 * g0/c0 * diag((1/R^2))
## prior on mu
b0 <- apply(y, 2, median)
B_0 <- rep(1, r)  #initial values for lambda are 1
B0 <- diag((R^2) * B_0)
nu <- 0.5
## initial values for parameters to be estimated:
eta_0 <- rep(1/K, K)
sigma_0 <- array(0, dim = c(r, r, K))
for (k in 1:K) {
sigma_0[, , k] <- C0
}
C0_0 <- C0
## initial classification
groups <- K
cl_y <- kmeans(y, centers = groups, nstart = 30)
S_0 <- cl_y$cluster
mu_0 <- cbind(t(cl_y$centers))
## generate matrices for saving the results:
Eta <- matrix(0, M, K)
Mu <- array(0, dim = c(M, r, K))
B <- matrix(0, M, r)
Eta_Matrix_FS <- matrix(0, r, K)
Sigma_Matrix_FS <- array(0, dim = c(r, r, K))
Mu_Matrix_FS <- array(0, dim = c(r, K))
#---------- C) Gibbs sampling from the posterior -----------------------------------------------
#print(B_0)
################ call MCMC procedure
############### run 2 parallel chains
estGibbs <- MultVar_NormMixt_Gibbs_IndPriorNormalgamma(y, S_0, mu_0, sigma_0, eta_0, e0, c0, C0_0,
g0, G0, b0, B0, nu, B_0, M, burnin, c_proposal, priorOnE0 = priorOnE0, lambda = FALSE,seed =seed_, sigma_py =  sigma_py)
estGibbs_2 <- MultVar_NormMixt_Gibbs_IndPriorNormalgamma(y, S_0, mu_0, sigma_0, eta_0, e0, c0, C0_0,
g0, G0, b0, B0, nu, B_0, M, burnin, c_proposal, priorOnE0 = priorOnE0, lambda = FALSE,seed =seed_+1, sigma_py =  sigma_py)
Mu <-estGibbs$Mu
Sigma <- estGibbs$Sigma
Eta <- estGibbs$Eta
S_alt_matrix <- estGibbs$S_alt_matrix
B <- estGibbs$B
e0_vector <- estGibbs$e0_vector
acc_rate <- estGibbs$acc_rate
Eta_combined =   rbind(estGibbs$Eta[(burnin+1):M,],estGibbs_2$Eta[(burnin+1):M,])
Mu_combined =   abind(estGibbs$Mu[(burnin+1):M,,],estGibbs_2$Mu[(burnin+1):M,,] ,along = 1 )
Sigma_combined =   abind(estGibbs$Sigma[(burnin+1):M,,,],estGibbs_2$Sigma[(burnin+1):M,,,] ,along = 1 )
Nk_matrix_alt <- estGibbs$Nk_matrix_alt
Nk_matrix_alt2 <- estGibbs_2$Nk_matrix_alt
nonnormpost_mode_list <- estGibbs$nonnormpost_mode_list
## comute log-likelihood for each chain
ll1<- compute_log_lik(K, y, M, burnin, Eta, Mu, Sigma)
ll2<- compute_log_lik(K, y, M, burnin, estGibbs_2$Eta,estGibbs_2$Mu,estGibbs_2$Sigma)
## covergence diagnostics
log_lik_combines <- mcmc.list(mcmc(ll1),mcmc(ll2))
Rhat_ll<- gelman.diag(log_lik_combines)$psrf[1]
## convergence diagnostic
#Sigma_es = apply(Sigma, c(2,3,4),effectiveSize)
#Sigma_es_mean = mean(Sigma_es)
#Mu_es = apply(Mu, c(2,3),effectiveSize)
#Mu_es_mean = mean(Mu_es)
##### number of nonempty components (nne_gr), after burnin:
nne_gr <- apply(Nk_matrix_alt2, 1, function(x) sum(x != 0))
table(nne_gr)
#---------- E) Identification of the mixture model -----------------------------------------------
##### estimating the number of nonempty components
K0_vector <- rowSums(Nk_matrix_alt[(burnin+1):M,] != 0)  #vector with number of non-empty groups of each iteration
K0_vector2 <- rowSums(Nk_matrix_alt2[(burnin+1):M,] != 0)  #vector with number of non-empty groups of each iteration
p_K0 <- tabulate(c(K0_vector,K0_vector2), K)
p_K0
#par(mfrow = c(1, 1))
#barplot(p_K0, names = 1:K, xlab = "number of non-empty groups K0", col = "green", ylab = "freq")
K0 <- which.max(p_K0)
K0  #mode K0 is the estimator for K_true
M0 <- sum(K0_vector == K0)
M0  #M0 draws have exactly K0 non-empty groups
df  = MCMC_function(data, e0=1, K=10, M=1000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
source("~/Documents/GitHub/BNPconsistency/scripts_for_figures/Gibbs_sampling_function.R")
MCMC_function(data, e0=1, K=10, M=1000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
df <- MCMC_function(data, e0=1, K=10, M=2000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
pk<- list()
N<- c()
R_h <- c()
W_non_sorted <- list()
W <- list()
Mu_mat <- list()
S_mat<- list()
comparison_py <-  function(ds_list,K_, M_it, nburn, alpha_py,sigmapy){
pk<- list()
N<- c()
R_h <- c()
W_non_sorted <- list()
W <- list()
Mu_mat <- list()
S_mat<- list()
for (i in 1:length(ds_list)){
data =  loadRData(ds_list[i])
pk[[i]] <- MCMC_function(data, e0=alpha_py, K=K_, M=M_it, burnin=nburn, priorOnE0 = FALSE, sigma_py = sigmapy)
N[i]<- dim(data$y)[1]
#Eta_ <- apply(pk[[1]]$Eta , 2, function(x) c(sort(x, decreasing = TRUE)))
Eta_<- matrix(NA, nrow =dim(pk[[i]]$Eta)[1],ncol =  dim(pk[[i]]$Eta)[2] )
for (j in 1:dim(pk[[i]]$Eta)[1]){ Eta_[j,] <- sort(pk[[i]]$Eta[j,],decreasing = TRUE)}
W[[i]] <- Eta_
W_non_sorted[[i]] <- pk[[i]]$Eta
Mu_mat[[i]] <- pk[[i]]$Mu
S_mat[[i]] <-pk[[i]]$Sigma
}
df_ = tibble(K= 1:K_)
df2_ = tibble(K= 1:K_)
df3_ = tibble(K= 1:K_)
df4_ = tibble(K= 1:K_)
for (j in 1:length(ds_list)){
name_ <- paste("Pkn_", j, sep = "")
name2_ <- paste("Rh_", j, sep = "")
name3_ <- paste("N_", j, sep = "")
name4_ <- paste("W_", j, sep = "")
df_[,name_]<- pk[[j]]$p_k
df2_[,name2_]<- rep(pk[[j]]$ll_rhat,length(pk[[j]]$p_k))
df3_[,name3_]<- rep(N[j],length(pk[[j]]$p_k))
df4_[,name4_]<- rep(N[j],length(pk[[j]]$p_k))
}
df = df_%>% gather(Process_type, density,  paste("Pkn_", 1, sep = ""):paste("Pkn_", length(ds_list), sep = ""))
df2 = df2_%>% gather(Rh, Rh_val,  paste("Rh_", 1, sep = ""):paste("Rh_", length(ds_list), sep = ""))
df3 = df3_%>% gather(N_, N_val,  paste("N_", 1, sep = ""):paste("N_", length(ds_list), sep = ""))
df4 = df4_%>% gather(W_, W_val,  paste("W_", 1, sep = ""):paste("W_", length(ds_list), sep = ""))
W_df <- do.call(cbind, W)
df4_post <- cbind(df4,t(W_df))
df4_post_ <- gather(df4_post, key = "it",value="weights", 4: dim(df4_post)[2])
df$alpha = c(rep(alpha,dim(df_)[1]*length(ds_list)))
df$Rh = df2$Rh_val
df$N =df3$N_val
df_l_ = tibble(K= 1:((M_it -nburn)*2))
df2_l_ = tibble(K= 1:((M_it - nburn)*2))
df3_l_ = tibble(K= 1:((M_it - nburn)*2))
for (j in 1:length(ds_list)){
name_ <- paste("P_", j, sep = "")
name2_ <- paste("Rh_", j, sep = "")
name3_ <- paste("N_", j, sep = "")
df_l_[,name_]<- pk[[j]]$p_k_all
df2_l_[,name2_]<- rep(pk[[j]]$ll_rhat,length(pk[[j]]$p_k_all))
df3_l_[,name3_]<- rep(N[j],length(pk[[j]]$p_k_all))
}
df_l = df_l_%>% gather(Process_type, density,  paste("P_", 1, sep = ""):paste("P_", length(ds_list), sep = ""))
df_l2 = df2_l_%>% gather(Rh, Rh_val,  paste("Rh_", 1, sep = ""):paste("Rh_", length(ds_list), sep = ""))
df_l3 = df3_l_%>% gather(N_, N_val,  paste("N_", 1, sep = ""):paste("N_", length(ds_list), sep = ""))
df_l$alpha = c(rep(alpha,(M_it -nburn)*2*length(ds_list)))
df_l$Rh = df_l2$Rh_val
df_l$N =df_l3$N_val
}
df <- comparison_py(ds_list,K_=10, M_it=1000, nburn=200, alpha_py=1,sigmapy = 0.25)
ds_list<- c("~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_200.RData",
"~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_2000.RData","~/Documents/GitHub/BNPconsistency/scripts_for_figures/sim_data/GM_3_20000.RData")
K_=10
M_it=1000
nburn=200
alpha_py=1
sigmapy = 0.25
pk<- list()
N<- c()
R_h <- c()
W_non_sorted <- list()
W <- list()
Mu_mat <- list()
S_mat<- list()
for (i in 1:length(ds_list)){
data =  loadRData(ds_list[i])
pk[[i]] <- MCMC_function(data, e0=alpha_py, K=K_, M=M_it, burnin=nburn, priorOnE0 = FALSE, sigma_py = sigmapy)
N[i]<- dim(data$y)[1]
#Eta_ <- apply(pk[[1]]$Eta , 2, function(x) c(sort(x, decreasing = TRUE)))
Eta_<- matrix(NA, nrow =dim(pk[[i]]$Eta)[1],ncol =  dim(pk[[i]]$Eta)[2] )
for (j in 1:dim(pk[[i]]$Eta)[1]){ Eta_[j,] <- sort(pk[[i]]$Eta[j,],decreasing = TRUE)}
W[[i]] <- Eta_
W_non_sorted[[i]] <- pk[[i]]$Eta
Mu_mat[[i]] <- pk[[i]]$Mu
S_mat[[i]] <-pk[[i]]$Sigma
}
data =  loadRData(ds_list[2])
df <- MCMC_function(data, e0=1, K=10, M=2000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
df <- MCMC_function(data, e0=1, K=10, M=2000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
data =  loadRData(ds_list[2])
df <- MCMC_function(data, e0=1, K=10, M=2000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
data =  loadRData(ds_list[3])
df <- MCMC_function(data, e0=1, K=10, M=2000, burnin=200,seed_ = 1000, priorOnE0 = FALSE, sigma_py = 0.25)
